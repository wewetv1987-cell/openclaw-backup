I keep going back and forth on AI coding tools. Last week I mass-accepted a bunch of Copilot suggestions in a config file and didn't notice until two days later that one of them referenced a package we'd already ripped out.

The thing is, they're genuinely useful for boilerplate. Test scaffolding, repetitive CRUD endpoints, config files you've written a hundred times before. I wrote a React form component last month that took maybe ten minutes with Copilot where it would've taken thirty without. Fine.

But I've also watched a junior dev on my team accept suggestions for three weeks straight and end up with code that worked, passed CI, and was completely unmaintainable. Nobody caught it until code review, and by then it was a 2000-line PR.

The productivity numbers are squishy. GitHub says 30% of suggestions get accepted, but that's not the same as 30% improvement. Some of those accepted suggestions still need editing. The Uplevel study from last year found basically no difference in PR throughput between teams with and without Copilot.

I still use it. I just don't trust it.
